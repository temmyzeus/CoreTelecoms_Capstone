import json
import os
import tempfile

from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from sqlalchemy import create_engine


def download_file_from_s3_to_local(
    aws_conn_id: str,
    key, 
    bucket_name, 
    local_path=None
):
    """
    Downloads a file from S3 to a local path.
    """
    s3_hook = S3Hook(aws_conn_id=aws_conn_id)
    filename = s3_hook.download_file(
        key=key,
        bucket_name=bucket_name,
        local_path=local_path,
        preserve_file_name=True,
        use_autogenerated_subdir=True
    )
    return filename

def upload_to_s3_as_parquet(
    aws_conn_id: str,
    bucket_name: str,
    s3_key: str,
    local_file_path: str = None,
    df = None,
    is_json: bool = False,
    mode: str = "overwrite"
):
    """
    Uploads a local CSV file or a pandas DataFrame to S3 as a Parquet file.
    """
    import awswrangler as wr
    import pandas as pd

    if (local_file_path) and (df):
        raise ValueError("Provide either local_file_path or df, not both.")

    if (df) and (not isinstance(df, pd.DataFrame)):
        raise ValueError("The provided df is not a valid pandas DataFrame.")
    
    wr.engine.set("python")

    if local_file_path and (not is_json):
        df = pd.read_csv(local_file_path)
    elif local_file_path and is_json:
        df = pd.read_json(local_file_path, orient="columns")

    # Upload the DataFrame to S3 as Parquet
    data_return_value = wr.s3.to_parquet(
        df=df,
        path=f"s3://{bucket_name}/{s3_key}",
        compression="snappy",
        dataset=True,
        mode=mode,
        boto3_session=S3Hook(aws_conn_id=aws_conn_id).get_session()
    )
    return data_return_value

def postgres_to_s3_as_parquet(
    aws_conn_id: str,
    postgres_conn_id: str,
    schema: str,
    table: str,
    bucket_name: str,
    s3_key: str,
    chunk_size: int = 50_000
):
    """
    Read a large table in chunks to avoid statement_timeout and save as parquet
    """

    import pandas as pd
    import awswrangler as wr

    pg_hook = PostgresHook(postgres_conn_id=postgres_conn_id)
    print(f"SQL Alchemy URI: {pg_hook.sqlalchemy_url}")
    engine = create_engine(pg_hook.sqlalchemy_url, connect_args={"options": "-c statement_timeout=300000"})

    s3_uri = os.path.join("s3://", bucket_name, s3_key)

    filename = os.path.basename(s3_uri)
    filename_without_ext, ext = os.path.splitext(filename)

    os.makedirs("/tmp/web_forms/", exist_ok=True)
    tmp_dir = tempfile.TemporaryDirectory(suffix=".parquet", prefix=f"/tmp/web_forms/{filename_without_ext}_", delete=False)
    tmp_dir = tmp_dir.name

    print(f"Reading {schema}.{table} in chunks of {chunk_size}")

    last_ctid = None
    saved_parquet_file_paths = []
    
    i = 0

    while True:
        if last_ctid is None:
            sql = f"""
                SELECT ctid, * FROM {schema}.{table}
                ORDER BY ctid
                LIMIT {chunk_size}
            """
        else:
            sql = f"""
                SELECT ctid, * FROM {schema}.{table}
                WHERE ctid > '{last_ctid}'
                ORDER BY ctid
                LIMIT {chunk_size}
            """
        
        df_chunk = pd.read_sql(sql, engine)
        if df_chunk.empty:
            break
            
        last_ctid = df_chunk['ctid'].iloc[-1]
        # Drop ctid before returning
        df_chunk = df_chunk.drop(columns=['ctid'])
        saved_parquet_file_path = os.path.join(tmp_dir, f"chunk_{i}.parquet")
        df_chunk.to_parquet(saved_parquet_file_path, index=False)
        i += 1
        saved_parquet_file_paths.append(saved_parquet_file_path)
        print(f"Fetched chunk ending at ctid={last_ctid}")

    for saved_file_path in saved_parquet_file_paths:
        save_as_s3_key = os.path.join(s3_key, os.path.basename(saved_file_path))
        S3Hook(aws_conn_id=aws_conn_id).load_file(saved_file_path, save_as_s3_key, bucket_name)
    
    return tmp_dir

def upload_google_sheet_to_s3_as_parquet(
    aws_conn_id: str,
    service_cred_json: str,
    sheet_id: str,
    sheet_title: str,
    bucket_name: str,
    s3_key: str,
    mode: str = "overwrite"
):
    import gspread
    from oauth2client.service_account import ServiceAccountCredentials
    import json
    import pandas as pd

    # Define API scopes
    scope = ['https://www.googleapis.com/auth/spreadsheets']

    # Path to your service account JSON key file
    creds_file = 'service.json'

    with open(creds_file, mode="w") as f:
        json.dump(service_cred_json, f, indent=4)

    # Authenticate with Google Sheets
    creds = ServiceAccountCredentials.from_json_keyfile_name(creds_file, scope)
    client = gspread.authorize(creds)

    # Open the Google Sheet by its title
    spreadsheet = client.open_by_key(sheet_id).worksheet(sheet_title)

    records = spreadsheet.get_all_records()

    with open('data.json', 'w') as f:
        json.dump(records, f)

    with open('data.json', 'r') as f:
        data = json.load(f)
        df = pd.DataFrame(data)

    upload_to_s3_as_parquet(
        aws_conn_id=aws_conn_id,
        bucket_name=bucket_name,
        s3_key=s3_key,
        df=df,
        mode=mode
    )
