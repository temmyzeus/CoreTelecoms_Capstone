import os

from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook


def download_file_from_s3_to_local(
    aws_conn_id: str,
    key, 
    bucket_name, 
    local_path=None
):
    """
    Downloads a file from S3 to a local path.
    """
    s3_hook = S3Hook(aws_conn_id=aws_conn_id)
    filename = s3_hook.download_file(
        key=key,
        bucket_name=bucket_name,
        local_path=local_path,
        preserve_file_name=True,
        use_autogenerated_subdir=True
    )
    return filename

def upload_to_s3_as_parquet(
    aws_conn_id: str,
    bucket_name: str,
    s3_key: str,
    local_file_path: str = None,
    df = None,
    is_json: bool = False,
    mode: str = "overwrite"
):
    """
    Uploads a local CSV file or a pandas DataFrame to S3 as a Parquet file.
    """
    import awswrangler as wr
    import pandas as pd

    if (local_file_path) and (df):
        raise ValueError("Provide either local_file_path or df, not both.")

    if (df) and (not isinstance(df, pd.DataFrame)):
        raise ValueError("The provided df is not a valid pandas DataFrame.")
    
    wr.engine.set("python")

    if local_file_path and (not is_json):
        df = pd.read_csv(local_file_path)
    elif local_file_path and is_json:
        df = pd.read_json(local_file_path, orient="columns")

    # Upload the DataFrame to S3 as Parquet
    data_return_value = wr.s3.to_parquet(
        df=df,
        path=f"s3://{bucket_name}/{s3_key}",
        compression="snappy",
        dataset=True,
        mode=mode,
        boto3_session=S3Hook(aws_conn_id=aws_conn_id).get_session()
    )
    return data_return_value

def postgres_to_s3_as_parquet(
    aws_conn_id: str,
    postgres_conn_id: str,
    sql_query: str,
    bucket_name: str,
    s3_key: str,
    mode: str = "overwrite"
):
    pg_hook = PostgresHook(postgres_conn_id=postgres_conn_id)
    
    data_return_value = upload_to_s3_as_parquet(
        aws_conn_id=aws_conn_id,
        bucket_name=bucket_name,
        s3_key=s3_key,
        df=pg_hook.get_df(sql_query),
        mode=mode
    )

    return data_return_value


def upload_google_sheet_to_s3_as_parquet(
    aws_conn_id: str,
    creds_file: str,
    sheet_id: str,
    sheet_title: str,
    bucket_name: str,
    s3_key: str,
    mode: str = "overwrite"
):
    import gspread
    from oauth2client.service_account import ServiceAccountCredentials
    import json
    import pandas as pd

    # Define API scopes
    scope = ['https://www.googleapis.com/auth/spreadsheets']

    # Path to your service account JSON key file
    creds_file = 'core-telecoms-service.json'

    # Authenticate with Google Sheets
    creds = ServiceAccountCredentials.from_json_keyfile_name(creds_file, scope)
    client = gspread.authorize(creds)

    # Open the Google Sheet by its title
    spreadsheet = client.open_by_key(sheet_id).worksheet(sheet_title)

    records = spreadsheet.get_all_records()

    with open('data.json', 'w') as f:
        json.dump(records, f)

    with open('data.json', 'r') as f:
        data = json.load(f)
        df = pd.DataFrame(data)

    upload_to_s3_as_parquet(
        aws_conn_id=aws_conn_id,
        bucket_name=bucket_name,
        s3_key=s3_key,
        df=df,
        mode=mode
    )
